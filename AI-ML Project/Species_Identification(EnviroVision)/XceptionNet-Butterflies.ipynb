{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1438b8ba-0aaa-4dfa-bfd5-4389cdc2f1dd",
   "metadata": {},
   "source": [
    "Odonata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207fb217-4c3c-42a7-bc96-3062272587bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Optimized Code for Bird Classification\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress AVX warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # Mitigate OpenMP conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084d451d-29b0-4108-950e-e9b3cb2c6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetV2L\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01189549-07a3-4030-84b5-8e068d6508cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data Augmentation with Mixup and CutMix\n",
    "class AugmentedGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, generator, alpha=0.4, cutmix_prob=0.5):\n",
    "        self.generator = generator\n",
    "        self.alpha = alpha\n",
    "        self.cutmix_prob = cutmix_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        images, labels = self.generator[index]\n",
    "        \n",
    "        if np.random.rand() < self.cutmix_prob:\n",
    "            return self.cutmix(images, labels)\n",
    "        else:\n",
    "            return self.mixup(images, labels)\n",
    "    \n",
    "    def mixup(self, images, labels):\n",
    "        indices = np.random.permutation(images.shape[0])\n",
    "        shuffled_images = images[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        mixed_images = lam * images + (1 - lam) * shuffled_images\n",
    "        mixed_labels = lam * labels + (1 - lam) * shuffled_labels\n",
    "        return mixed_images, mixed_labels\n",
    "\n",
    "    def cutmix(self, images, labels):\n",
    "        indices = np.random.permutation(images.shape[0])\n",
    "        shuffled_images = images[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(images.shape, lam)\n",
    "        images[:, bbx1:bbx2, bby1:bby2, :] = shuffled_images[:, bbx1:bbx2, bby1:bby2, :]\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.shape[1] * images.shape[2]))\n",
    "        labels = lam * labels + (1 - lam) * shuffled_labels\n",
    "        return images, labels\n",
    "\n",
    "    def rand_bbox(self, size, lam):\n",
    "        W = size[1]\n",
    "        H = size[2]\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "        return bbx1, bby1, bbx2, bby2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62626bdf-5ebe-45ce-b63a-cba52911878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths (UPDATE THESE TO YOUR LOCAL PATHS)\n",
    "train_path = 'D:/Dataset/Butterflies/Train'\n",
    "test_path = 'D:/Dataset/Butterflies/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a673f3d9-43f8-4f1f-8673-936b5d392d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85 images belonging to 12 classes.\n",
      "Found 13 images belonging to 12 classes.\n",
      "Found 22 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Generators with Validation Split\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input,\n",
    "    rotation_range=60,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    shear_range=0.4,\n",
    "    zoom_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.4, 1.6],\n",
    "    validation_split=0.2  # 80% training, 20% validation\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(480, 480),\n",
    "    batch_size=4,  # Reduced batch size for memory stability\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(480, 480),\n",
    "    batch_size=4,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(480, 480),\n",
    "    batch_size=4,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a07d06f-ca78-4b9b-8ca3-f20f13979fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Enhanced Model Architecture\n",
    "base_model = EfficientNetV2L(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(480, 480, 3)\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    BatchNormalization(),\n",
    "    Dense(2048, activation='swish', kernel_regularizer='l2'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1024, activation='swish', kernel_regularizer='l2'),\n",
    "    Dropout(0.4),\n",
    "    Dense(12, activation='softmax')  # Ensure this matches your class count\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c32a6a-2fbf-4d49-8e09-ad9792074b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE THE MODEL BEFORE TRAINING\n",
    "optimizer = Adam(learning_rate=1e-4, amsgrad=True)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "013c590d-7807-4523-8345-ae760c281f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 126s 4s/step - loss: 31.6931 - accuracy: 0.1647 - val_loss: 31.2876 - val_accuracy: 0.5385\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 88s 4s/step - loss: 30.9873 - accuracy: 0.2235 - val_loss: 30.8045 - val_accuracy: 0.5385\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 89s 4s/step - loss: 30.4861 - accuracy: 0.3412 - val_loss: 30.4087 - val_accuracy: 0.6154\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 91s 4s/step - loss: 29.9185 - accuracy: 0.4588 - val_loss: 29.9307 - val_accuracy: 0.5385\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 88s 4s/step - loss: 29.4211 - accuracy: 0.5412 - val_loss: 29.5648 - val_accuracy: 0.5385\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 91s 4s/step - loss: 29.0643 - accuracy: 0.6118 - val_loss: 29.2167 - val_accuracy: 0.5385\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 90s 4s/step - loss: 28.6762 - accuracy: 0.5882 - val_loss: 28.6178 - val_accuracy: 0.7692\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 92s 4s/step - loss: 28.5015 - accuracy: 0.4118 - val_loss: 28.1752 - val_accuracy: 0.6923\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 92s 4s/step - loss: 27.7965 - accuracy: 0.6000 - val_loss: 27.7605 - val_accuracy: 0.7692\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 91s 4s/step - loss: 27.2553 - accuracy: 0.7882 - val_loss: 27.3586 - val_accuracy: 0.7692\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 91s 4s/step - loss: 27.3189 - accuracy: 0.5294 - val_loss: 26.7975 - val_accuracy: 0.8462\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 92s 4s/step - loss: 26.8538 - accuracy: 0.6235 - val_loss: 26.4373 - val_accuracy: 0.6923\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 26.1655 - accuracy: 0.7529 - val_loss: 25.9276 - val_accuracy: 0.8462\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 92s 4s/step - loss: 25.9561 - accuracy: 0.6000 - val_loss: 25.4730 - val_accuracy: 0.8462\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 25.8912 - accuracy: 0.5412 - val_loss: 25.2119 - val_accuracy: 0.6923\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 95s 4s/step - loss: 25.2948 - accuracy: 0.6000 - val_loss: 24.7874 - val_accuracy: 0.8462\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 94s 4s/step - loss: 24.9108 - accuracy: 0.6588 - val_loss: 24.2249 - val_accuracy: 0.8462\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 92s 4s/step - loss: 24.3559 - accuracy: 0.6941 - val_loss: 23.9745 - val_accuracy: 0.7692\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 24.1075 - accuracy: 0.7059 - val_loss: 23.5386 - val_accuracy: 0.9231\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 92s 4s/step - loss: 23.6172 - accuracy: 0.7294 - val_loss: 23.2718 - val_accuracy: 0.8462\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 94s 4s/step - loss: 23.2975 - accuracy: 0.7529 - val_loss: 22.7982 - val_accuracy: 0.8462\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 23.1595 - accuracy: 0.6824 - val_loss: 22.3399 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 22.8489 - accuracy: 0.6824 - val_loss: 22.2071 - val_accuracy: 0.7692\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 22.1531 - accuracy: 0.7647 - val_loss: 21.8916 - val_accuracy: 0.7692\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 94s 4s/step - loss: 21.9090 - accuracy: 0.7765 - val_loss: 21.4210 - val_accuracy: 0.7692\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 95s 4s/step - loss: 21.5560 - accuracy: 0.7176 - val_loss: 21.1999 - val_accuracy: 0.7692\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 91s 4s/step - loss: 21.5855 - accuracy: 0.6235 - val_loss: 20.6524 - val_accuracy: 0.8462\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 92s 4s/step - loss: 21.0769 - accuracy: 0.7059 - val_loss: 20.4838 - val_accuracy: 0.8462\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 92s 4s/step - loss: 20.6942 - accuracy: 0.7294 - val_loss: 20.2548 - val_accuracy: 0.6923\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 20.4398 - accuracy: 0.7294 - val_loss: 19.6624 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 20.0383 - accuracy: 0.6706 - val_loss: 19.4737 - val_accuracy: 0.8462\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 19.7574 - accuracy: 0.7059 - val_loss: 18.9067 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 19.4061 - accuracy: 0.7647 - val_loss: 18.7005 - val_accuracy: 0.8462\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 94s 4s/step - loss: 19.2297 - accuracy: 0.6471 - val_loss: 18.4304 - val_accuracy: 0.9231\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 18.7983 - accuracy: 0.7059 - val_loss: 18.3512 - val_accuracy: 0.6154\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 92s 4s/step - loss: 18.6192 - accuracy: 0.6706 - val_loss: 17.7509 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 92s 4s/step - loss: 18.3192 - accuracy: 0.6471 - val_loss: 17.3787 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 17.7366 - accuracy: 0.8235 - val_loss: 17.0717 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 95s 4s/step - loss: 17.5454 - accuracy: 0.7412 - val_loss: 16.9376 - val_accuracy: 0.7692\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 17.3584 - accuracy: 0.7059 - val_loss: 16.5412 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 16.8762 - accuracy: 0.7647 - val_loss: 16.2945 - val_accuracy: 0.8462\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 94s 4s/step - loss: 16.8794 - accuracy: 0.6941 - val_loss: 15.9459 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 91s 4s/step - loss: 16.3777 - accuracy: 0.7882 - val_loss: 15.9130 - val_accuracy: 0.7692\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 16.3697 - accuracy: 0.6706 - val_loss: 15.4693 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 15.8773 - accuracy: 0.7647 - val_loss: 15.3771 - val_accuracy: 0.8462\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 93s 4s/step - loss: 15.6424 - accuracy: 0.7059 - val_loss: 14.9542 - val_accuracy: 0.9231\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 94s 4s/step - loss: 15.5118 - accuracy: 0.7412 - val_loss: 14.5511 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 94s 4s/step - loss: 15.0452 - accuracy: 0.8235 - val_loss: 14.5255 - val_accuracy: 0.8462\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 94s 4s/step - loss: 14.8889 - accuracy: 0.7176 - val_loss: 14.0635 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 91s 4s/step - loss: 14.7457 - accuracy: 0.7412 - val_loss: 14.0028 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# First Training Phase (Frozen Backbone)\n",
    "history = model.fit(\n",
    "    AugmentedGenerator(train_generator),\n",
    "    validation_data=val_generator,\n",
    "    epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50021383-16c0-4306-9d7e-58dcbfa11298",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:/Model_Main/Xception_net_client_Butterflies_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62304743-2b04-4ee9-b7f5-526afb52df8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/79\n",
      "22/22 [==============================] - 273s 10s/step - loss: 14.8036 - accuracy: 0.6235 - val_loss: 14.1790 - val_accuracy: 0.8462\n",
      "Epoch 51/79\n",
      "22/22 [==============================] - 203s 9s/step - loss: 14.7544 - accuracy: 0.6588 - val_loss: 14.0036 - val_accuracy: 0.8462\n",
      "Epoch 52/79\n",
      "22/22 [==============================] - 202s 9s/step - loss: 14.5525 - accuracy: 0.7059 - val_loss: 13.8601 - val_accuracy: 0.9231\n",
      "Epoch 53/79\n",
      "22/22 [==============================] - 204s 9s/step - loss: 14.3496 - accuracy: 0.7765 - val_loss: 13.7087 - val_accuracy: 1.0000\n",
      "Epoch 54/79\n",
      "22/22 [==============================] - 206s 9s/step - loss: 14.4647 - accuracy: 0.8353 - val_loss: 13.8160 - val_accuracy: 0.9231\n",
      "Epoch 55/79\n",
      "22/22 [==============================] - 202s 9s/step - loss: 14.3437 - accuracy: 0.7647 - val_loss: 13.8083 - val_accuracy: 0.8462\n",
      "Epoch 56/79\n",
      "22/22 [==============================] - 204s 9s/step - loss: 14.3117 - accuracy: 0.8941 - val_loss: 13.7187 - val_accuracy: 0.9231\n",
      "Epoch 57/79\n",
      "22/22 [==============================] - 206s 9s/step - loss: 14.2095 - accuracy: 0.8000 - val_loss: 13.5395 - val_accuracy: 1.0000\n",
      "Epoch 58/79\n",
      "22/22 [==============================] - 202s 9s/step - loss: 14.2376 - accuracy: 0.7882 - val_loss: 13.5686 - val_accuracy: 0.9231\n",
      "Epoch 59/79\n",
      "22/22 [==============================] - 206s 9s/step - loss: 14.1976 - accuracy: 0.8118 - val_loss: 13.6368 - val_accuracy: 0.9231\n",
      "Epoch 60/79\n",
      "22/22 [==============================] - 206s 9s/step - loss: 14.2891 - accuracy: 0.7059 - val_loss: 13.5923 - val_accuracy: 1.0000\n",
      "Epoch 61/79\n",
      "22/22 [==============================] - 204s 9s/step - loss: 14.0131 - accuracy: 0.8588 - val_loss: 13.4487 - val_accuracy: 1.0000\n",
      "Epoch 62/79\n",
      "22/22 [==============================] - 202s 9s/step - loss: 13.9687 - accuracy: 0.8824 - val_loss: 13.4454 - val_accuracy: 1.0000\n",
      "Epoch 63/79\n",
      "22/22 [==============================] - 203s 9s/step - loss: 13.8471 - accuracy: 0.9176 - val_loss: 13.3953 - val_accuracy: 0.9231\n",
      "Epoch 64/79\n",
      "22/22 [==============================] - 204s 9s/step - loss: 14.0318 - accuracy: 0.8353 - val_loss: 13.3524 - val_accuracy: 0.9231\n",
      "Epoch 65/79\n",
      "22/22 [==============================] - 199s 9s/step - loss: nan - accuracy: 0.2000 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 66/79\n",
      "22/22 [==============================] - 197s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 67/79\n",
      "22/22 [==============================] - 198s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 68/79\n",
      "22/22 [==============================] - 199s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 69/79\n",
      "22/22 [==============================] - 198s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 70/79\n",
      "22/22 [==============================] - 196s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 71/79\n",
      "22/22 [==============================] - 198s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 72/79\n",
      "22/22 [==============================] - 199s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 73/79\n",
      "22/22 [==============================] - 198s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 74/79\n",
      "22/22 [==============================] - 196s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 75/79\n",
      "22/22 [==============================] - 197s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 76/79\n",
      "22/22 [==============================] - 199s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 77/79\n",
      "22/22 [==============================] - 198s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 78/79\n",
      "22/22 [==============================] - 196s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n",
      "Epoch 79/79\n",
      "22/22 [==============================] - 201s 9s/step - loss: nan - accuracy: 0.0824 - val_loss: nan - val_accuracy: 0.0769\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Phase\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:300]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_fine = model.fit(\n",
    "    AugmentedGenerator(train_generator),\n",
    "    validation_data=val_generator,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    epochs=history.epoch[-1] + 30,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9abfc2b3-2e5b-41c7-91b9-b518f08aee13",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.62 MiB for an array with shape (1, 1, 2304, 640) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/Model_Main/Xception_net_client_Butterflies_cnn_tunned.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\evenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\evenv\\lib\\site-packages\\keras\\backend.py:3968\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3956\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n\u001b[0;32m   3957\u001b[0m \n\u001b[0;32m   3958\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3965\u001b[0m \u001b[38;5;124;03m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n\u001b[0;32m   3966\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m-> 3968\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tensors]\n\u001b[0;32m   3969\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39minside_function():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   3970\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.62 MiB for an array with shape (1, 1, 2304, 640) and data type float32"
     ]
    }
   ],
   "source": [
    "model.save('D:/Model_Main/Xception_net_client_Butterflies_cnn_tunned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "320a6bad-febb-49e9-8da7-6972ea1b6d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 16s 3s/step - loss: nan - accuracy: 0.0909\n",
      "Test Loss: nan | Test Accuracy: 0.0909\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {eval_results[0]:.4f} | Test Accuracy: {eval_results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ce889-186c-4d03-8693-0f384121838b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa87a14-fe24-4bff-ab17-ef4154bfd8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f7744-6f6d-4af4-9438-8cef9df43f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04201eba-d207-44bd-b4d0-5a6cab6e62b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetv2-l (Functiona  (None, 15, 15, 1280)     117746848 \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 1280)             5120      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              2623488   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 9)                 9225      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,482,857\n",
      "Trainable params: 113,256,857\n",
      "Non-trainable params: 9,226,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Path to your saved model\n",
    "model_path = 'D:/Model_Main/Xception_net_client_Butterflies_cnn2.h5'\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Optional: Print model summary to confirm it's loaded correctly\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afa85903-071b-43e6-86a9-d3c364fb82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 21s 3s/step - loss: 16.1888 - accuracy: 0.8889\n",
      "Test Loss: 16.1888 | Test Accuracy: 0.8889\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {eval_results[0]:.4f} | Test Accuracy: {eval_results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeeeaffa-ba0e-4b97-9ffe-f0d964c08ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels manually\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class_labels = {\n",
    "    0: 'Bushybrow',\n",
    "    1: 'Cinnamon crow',\n",
    "    2: 'Dark Tiger',\n",
    "    3: 'Great Mormon',\n",
    "    4: 'Grey Pansy',\n",
    "    5: 'Lacewing',\n",
    "    6: 'Lemon Pansy',\n",
    "    7: 'Mormon',\n",
    "    8: 'Painted bell',\n",
    "    9: 'Peacock pansy',\n",
    "    10: 'Sailer',\n",
    "    11: 'Swoard Tail'\n",
    "}\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('D:/Model_Main/Xception_net_client_Butterflies_cnn.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc33a3c8-1a4f-45d2-9997-febf21ebe701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "The predicted class is: Swoard Tail\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = 'D:/Less_data_test/Butterflies/b38.jpg'\n",
    "test_image = load_img(image_path, target_size=(480, 480))  # Match model input size\n",
    "test_image = img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "test_image = preprocess_input(test_image)  # Use EfficientNetV2 preprocessing\n",
    "\n",
    "# Predict the class\n",
    "result = model.predict(test_image)\n",
    "predicted_class = np.argmax(result)\n",
    "\n",
    "# Get the predicted label\n",
    "prediction = class_labels[predicted_class]\n",
    "\n",
    "print(f'The predicted class is: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de613a34-f14b-4c35-bad6-dae9ae7c46f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
