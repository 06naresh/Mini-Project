{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1438b8ba-0aaa-4dfa-bfd5-4389cdc2f1dd",
   "metadata": {},
   "source": [
    "Odonata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207fb217-4c3c-42a7-bc96-3062272587bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Optimized Code for Bird Classification\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress AVX warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'  # Mitigate OpenMP conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "084d451d-29b0-4108-950e-e9b3cb2c6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetV2L\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01189549-07a3-4030-84b5-8e068d6508cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data Augmentation with Mixup and CutMix\n",
    "class AugmentedGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, generator, alpha=0.4, cutmix_prob=0.5):\n",
    "        self.generator = generator\n",
    "        self.alpha = alpha\n",
    "        self.cutmix_prob = cutmix_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        images, labels = self.generator[index]\n",
    "        \n",
    "        if np.random.rand() < self.cutmix_prob:\n",
    "            return self.cutmix(images, labels)\n",
    "        else:\n",
    "            return self.mixup(images, labels)\n",
    "    \n",
    "    def mixup(self, images, labels):\n",
    "        indices = np.random.permutation(images.shape[0])\n",
    "        shuffled_images = images[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        mixed_images = lam * images + (1 - lam) * shuffled_images\n",
    "        mixed_labels = lam * labels + (1 - lam) * shuffled_labels\n",
    "        return mixed_images, mixed_labels\n",
    "\n",
    "    def cutmix(self, images, labels):\n",
    "        indices = np.random.permutation(images.shape[0])\n",
    "        shuffled_images = images[indices]\n",
    "        shuffled_labels = labels[indices]\n",
    "        lam = np.random.beta(self.alpha, self.alpha)\n",
    "        bbx1, bby1, bbx2, bby2 = self.rand_bbox(images.shape, lam)\n",
    "        images[:, bbx1:bbx2, bby1:bby2, :] = shuffled_images[:, bbx1:bbx2, bby1:bby2, :]\n",
    "        lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.shape[1] * images.shape[2]))\n",
    "        labels = lam * labels + (1 - lam) * shuffled_labels\n",
    "        return images, labels\n",
    "\n",
    "    def rand_bbox(self, size, lam):\n",
    "        W = size[1]\n",
    "        H = size[2]\n",
    "        cut_rat = np.sqrt(1. - lam)\n",
    "        cut_w = int(W * cut_rat)\n",
    "        cut_h = int(H * cut_rat)\n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "        return bbx1, bby1, bbx2, bby2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62626bdf-5ebe-45ce-b63a-cba52911878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths (UPDATE THESE TO YOUR LOCAL PATHS)\n",
    "train_path = 'D:/Dataset/Odonata/Train'\n",
    "test_path = 'D:/Dataset/Odonata/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a673f3d9-43f8-4f1f-8673-936b5d392d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 105 images belonging to 15 classes.\n",
      "Found 15 images belonging to 15 classes.\n",
      "Found 30 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Generators with Validation Split\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input,\n",
    "    rotation_range=60,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    shear_range=0.4,\n",
    "    zoom_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.4, 1.6],\n",
    "    validation_split=0.2  # 80% training, 20% validation\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(480, 480),\n",
    "    batch_size=4,  # Reduced batch size for memory stability\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(480, 480),\n",
    "    batch_size=4,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input\n",
    ")\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=(480, 480),\n",
    "    batch_size=4,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a07d06f-ca78-4b9b-8ca3-f20f13979fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Enhanced Model Architecture\n",
    "base_model = EfficientNetV2L(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(480, 480, 3)\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    BatchNormalization(),\n",
    "    Dense(2048, activation='swish', kernel_regularizer='l2'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1024, activation='swish', kernel_regularizer='l2'),\n",
    "    Dropout(0.4),\n",
    "    Dense(15, activation='softmax')  # Ensure this matches your class count\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c32a6a-2fbf-4d49-8e09-ad9792074b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPILE THE MODEL BEFORE TRAINING\n",
    "optimizer = Adam(learning_rate=1e-4, amsgrad=True)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "013c590d-7807-4523-8345-ae760c281f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "27/27 [==============================] - 140s 4s/step - loss: 32.0842 - accuracy: 0.0381 - val_loss: 31.4727 - val_accuracy: 0.2000\n",
      "Epoch 2/50\n",
      "27/27 [==============================] - 102s 4s/step - loss: 31.3634 - accuracy: 0.1429 - val_loss: 31.0119 - val_accuracy: 0.0667\n",
      "Epoch 3/50\n",
      "27/27 [==============================] - 104s 4s/step - loss: 30.6970 - accuracy: 0.2190 - val_loss: 30.5622 - val_accuracy: 0.0667\n",
      "Epoch 4/50\n",
      "27/27 [==============================] - 110s 4s/step - loss: 30.1822 - accuracy: 0.2762 - val_loss: 30.1106 - val_accuracy: 0.2667\n",
      "Epoch 5/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 29.7847 - accuracy: 0.2857 - val_loss: 29.6454 - val_accuracy: 0.4000\n",
      "Epoch 6/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 29.3224 - accuracy: 0.2762 - val_loss: 29.2348 - val_accuracy: 0.3333\n",
      "Epoch 7/50\n",
      "27/27 [==============================] - 106s 4s/step - loss: 28.8865 - accuracy: 0.3048 - val_loss: 28.8375 - val_accuracy: 0.0667\n",
      "Epoch 8/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 28.4533 - accuracy: 0.3238 - val_loss: 28.2821 - val_accuracy: 0.4667\n",
      "Epoch 9/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 28.0821 - accuracy: 0.3714 - val_loss: 27.8982 - val_accuracy: 0.4000\n",
      "Epoch 10/50\n",
      "27/27 [==============================] - 106s 4s/step - loss: 27.4741 - accuracy: 0.3524 - val_loss: 27.3993 - val_accuracy: 0.4000\n",
      "Epoch 11/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 27.1716 - accuracy: 0.3714 - val_loss: 26.9794 - val_accuracy: 0.4000\n",
      "Epoch 12/50\n",
      "27/27 [==============================] - 107s 4s/step - loss: 26.7876 - accuracy: 0.3714 - val_loss: 26.6178 - val_accuracy: 0.4000\n",
      "Epoch 13/50\n",
      "27/27 [==============================] - 107s 4s/step - loss: 26.2739 - accuracy: 0.4000 - val_loss: 26.0732 - val_accuracy: 0.4000\n",
      "Epoch 14/50\n",
      "27/27 [==============================] - 111s 4s/step - loss: 25.7836 - accuracy: 0.5048 - val_loss: 25.7119 - val_accuracy: 0.5333\n",
      "Epoch 15/50\n",
      "27/27 [==============================] - 118s 4s/step - loss: 25.3910 - accuracy: 0.4476 - val_loss: 25.4193 - val_accuracy: 0.5333\n",
      "Epoch 16/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 25.1917 - accuracy: 0.3905 - val_loss: 24.8858 - val_accuracy: 0.4000\n",
      "Epoch 17/50\n",
      "27/27 [==============================] - 109s 4s/step - loss: 24.7222 - accuracy: 0.4286 - val_loss: 24.6392 - val_accuracy: 0.3333\n",
      "Epoch 18/50\n",
      "27/27 [==============================] - 109s 4s/step - loss: 24.1437 - accuracy: 0.6000 - val_loss: 24.1383 - val_accuracy: 0.3333\n",
      "Epoch 19/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 23.9772 - accuracy: 0.4762 - val_loss: 23.5521 - val_accuracy: 0.4000\n",
      "Epoch 20/50\n",
      "27/27 [==============================] - 107s 4s/step - loss: 23.7483 - accuracy: 0.3905 - val_loss: 23.3304 - val_accuracy: 0.4667\n",
      "Epoch 21/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 23.1705 - accuracy: 0.4286 - val_loss: 23.0911 - val_accuracy: 0.4000\n",
      "Epoch 22/50\n",
      "27/27 [==============================] - 109s 4s/step - loss: 22.7404 - accuracy: 0.5619 - val_loss: 22.7173 - val_accuracy: 0.3333\n",
      "Epoch 23/50\n",
      "27/27 [==============================] - 109s 4s/step - loss: 22.2542 - accuracy: 0.5810 - val_loss: 22.1182 - val_accuracy: 0.4667\n",
      "Epoch 24/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 22.1213 - accuracy: 0.5238 - val_loss: 21.7814 - val_accuracy: 0.3333\n",
      "Epoch 25/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 21.7334 - accuracy: 0.5524 - val_loss: 21.5251 - val_accuracy: 0.4667\n",
      "Epoch 26/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 21.4523 - accuracy: 0.4571 - val_loss: 21.7307 - val_accuracy: 0.2667\n",
      "Epoch 27/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 21.0467 - accuracy: 0.4952 - val_loss: 20.8191 - val_accuracy: 0.4000\n",
      "Epoch 28/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 20.6794 - accuracy: 0.5619 - val_loss: 20.6776 - val_accuracy: 0.5333\n",
      "Epoch 29/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 20.3488 - accuracy: 0.5333 - val_loss: 20.0306 - val_accuracy: 0.6000\n",
      "Epoch 30/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 19.9082 - accuracy: 0.5429 - val_loss: 19.7522 - val_accuracy: 0.5333\n",
      "Epoch 31/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 19.6306 - accuracy: 0.5810 - val_loss: 19.2955 - val_accuracy: 0.6000\n",
      "Epoch 32/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 19.2540 - accuracy: 0.5714 - val_loss: 19.2463 - val_accuracy: 0.6000\n",
      "Epoch 33/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 19.0054 - accuracy: 0.6190 - val_loss: 18.5747 - val_accuracy: 0.6667\n",
      "Epoch 34/50\n",
      "27/27 [==============================] - 111s 4s/step - loss: 18.7489 - accuracy: 0.5714 - val_loss: 18.6306 - val_accuracy: 0.3333\n",
      "Epoch 35/50\n",
      "27/27 [==============================] - 113s 4s/step - loss: 18.4838 - accuracy: 0.4857 - val_loss: 18.4521 - val_accuracy: 0.4000\n",
      "Epoch 36/50\n",
      "27/27 [==============================] - 111s 4s/step - loss: 18.1053 - accuracy: 0.5810 - val_loss: 18.0466 - val_accuracy: 0.3333\n",
      "Epoch 37/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 17.9178 - accuracy: 0.5429 - val_loss: 17.6116 - val_accuracy: 0.4667\n",
      "Epoch 38/50\n",
      "27/27 [==============================] - 109s 4s/step - loss: 17.4621 - accuracy: 0.6286 - val_loss: 17.4338 - val_accuracy: 0.4667\n",
      "Epoch 39/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 17.1520 - accuracy: 0.5905 - val_loss: 16.9370 - val_accuracy: 0.5333\n",
      "Epoch 40/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 16.8231 - accuracy: 0.5810 - val_loss: 16.7682 - val_accuracy: 0.4667\n",
      "Epoch 41/50\n",
      "27/27 [==============================] - 107s 4s/step - loss: 16.6842 - accuracy: 0.6286 - val_loss: 16.2937 - val_accuracy: 0.4000\n",
      "Epoch 42/50\n",
      "27/27 [==============================] - 106s 4s/step - loss: 16.2868 - accuracy: 0.6286 - val_loss: 16.0811 - val_accuracy: 0.7333\n",
      "Epoch 43/50\n",
      "27/27 [==============================] - 109s 4s/step - loss: 16.1847 - accuracy: 0.5524 - val_loss: 16.0215 - val_accuracy: 0.4667\n",
      "Epoch 44/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 15.7697 - accuracy: 0.6190 - val_loss: 15.6565 - val_accuracy: 0.6000\n",
      "Epoch 45/50\n",
      "27/27 [==============================] - 111s 4s/step - loss: 15.5283 - accuracy: 0.6095 - val_loss: 15.6160 - val_accuracy: 0.4000\n",
      "Epoch 46/50\n",
      "27/27 [==============================] - 108s 4s/step - loss: 15.3787 - accuracy: 0.5524 - val_loss: 15.0990 - val_accuracy: 0.6000\n",
      "Epoch 47/50\n",
      "27/27 [==============================] - 110s 4s/step - loss: 14.9791 - accuracy: 0.6190 - val_loss: 14.8524 - val_accuracy: 0.5333\n",
      "Epoch 48/50\n",
      "27/27 [==============================] - 110s 4s/step - loss: 14.7592 - accuracy: 0.6286 - val_loss: 14.6893 - val_accuracy: 0.4667\n",
      "Epoch 49/50\n",
      "27/27 [==============================] - 111s 4s/step - loss: 14.3669 - accuracy: 0.6857 - val_loss: 14.3159 - val_accuracy: 0.5333\n",
      "Epoch 50/50\n",
      "27/27 [==============================] - 109s 4s/step - loss: 14.3308 - accuracy: 0.6190 - val_loss: 13.9564 - val_accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# First Training Phase (Frozen Backbone)\n",
    "history = model.fit(\n",
    "    AugmentedGenerator(train_generator),\n",
    "    validation_data=val_generator,\n",
    "    epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50021383-16c0-4306-9d7e-58dcbfa11298",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:/Model_Main/Xception_net_client_odonata_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62304743-2b04-4ee9-b7f5-526afb52df8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/79\n",
      "27/27 [==============================] - 302s 10s/step - loss: 14.4507 - accuracy: 0.4571 - val_loss: 13.6379 - val_accuracy: 0.4667\n",
      "Epoch 51/79\n",
      "27/27 [==============================] - 255s 9s/step - loss: 14.2477 - accuracy: 0.5810 - val_loss: 13.8203 - val_accuracy: 0.5333\n",
      "Epoch 52/79\n",
      "27/27 [==============================] - 252s 9s/step - loss: 14.1826 - accuracy: 0.5143 - val_loss: 13.6508 - val_accuracy: 0.6667\n",
      "Epoch 53/79\n",
      "27/27 [==============================] - 256s 9s/step - loss: 14.1332 - accuracy: 0.6000 - val_loss: 13.6733 - val_accuracy: 0.6667\n",
      "Epoch 54/79\n",
      "27/27 [==============================] - 254s 10s/step - loss: 13.9669 - accuracy: 0.6476 - val_loss: 13.7287 - val_accuracy: 0.5333\n",
      "Epoch 55/79\n",
      "27/27 [==============================] - 258s 10s/step - loss: 13.7532 - accuracy: 0.7524 - val_loss: 13.4641 - val_accuracy: 0.7333\n",
      "Epoch 56/79\n",
      "27/27 [==============================] - 257s 9s/step - loss: 13.8624 - accuracy: 0.7143 - val_loss: 13.2887 - val_accuracy: 0.8000\n",
      "Epoch 57/79\n",
      "27/27 [==============================] - 259s 10s/step - loss: 13.7976 - accuracy: 0.7143 - val_loss: 13.4241 - val_accuracy: 0.6667\n",
      "Epoch 58/79\n",
      "27/27 [==============================] - 260s 10s/step - loss: 13.7625 - accuracy: 0.7429 - val_loss: 13.3620 - val_accuracy: 0.7333\n",
      "Epoch 59/79\n",
      "27/27 [==============================] - 257s 10s/step - loss: 13.6760 - accuracy: 0.7524 - val_loss: 13.0268 - val_accuracy: 0.8667\n",
      "Epoch 60/79\n",
      "27/27 [==============================] - 253s 9s/step - loss: 13.3410 - accuracy: 0.8095 - val_loss: 13.2126 - val_accuracy: 0.6000\n",
      "Epoch 61/79\n",
      "27/27 [==============================] - 252s 9s/step - loss: 13.4794 - accuracy: 0.7429 - val_loss: 13.0502 - val_accuracy: 0.8667\n",
      "Epoch 62/79\n",
      "27/27 [==============================] - 256s 9s/step - loss: 13.4718 - accuracy: 0.7714 - val_loss: 13.1468 - val_accuracy: 0.7333\n",
      "Epoch 63/79\n",
      "27/27 [==============================] - 294s 11s/step - loss: 13.5963 - accuracy: 0.7143 - val_loss: 12.9891 - val_accuracy: 0.8000\n",
      "Epoch 64/79\n",
      "27/27 [==============================] - 369s 14s/step - loss: 13.2420 - accuracy: 0.8762 - val_loss: 13.2381 - val_accuracy: 0.8000\n",
      "Epoch 65/79\n",
      "27/27 [==============================] - 396s 15s/step - loss: 13.4788 - accuracy: 0.7619 - val_loss: 12.9764 - val_accuracy: 0.8000\n",
      "Epoch 66/79\n",
      "27/27 [==============================] - 365s 13s/step - loss: 13.1937 - accuracy: 0.8762 - val_loss: 12.8890 - val_accuracy: 0.8000\n",
      "Epoch 67/79\n",
      "27/27 [==============================] - 362s 13s/step - loss: 13.1035 - accuracy: 0.8381 - val_loss: 12.6958 - val_accuracy: 0.8667\n",
      "Epoch 68/79\n",
      "27/27 [==============================] - 274s 10s/step - loss: 13.0968 - accuracy: 0.8857 - val_loss: 12.6399 - val_accuracy: 0.8667\n",
      "Epoch 69/79\n",
      "27/27 [==============================] - 256s 9s/step - loss: 13.2580 - accuracy: 0.8381 - val_loss: 12.6962 - val_accuracy: 0.9333\n",
      "Epoch 70/79\n",
      "27/27 [==============================] - 252s 9s/step - loss: 12.9295 - accuracy: 0.8952 - val_loss: 12.7099 - val_accuracy: 0.9333\n",
      "Epoch 71/79\n",
      "27/27 [==============================] - 249s 9s/step - loss: 13.1173 - accuracy: 0.8190 - val_loss: 12.6891 - val_accuracy: 0.8000\n",
      "Epoch 72/79\n",
      "27/27 [==============================] - 248s 9s/step - loss: 12.9994 - accuracy: 0.8190 - val_loss: 12.5941 - val_accuracy: 0.8667\n",
      "Epoch 73/79\n",
      "27/27 [==============================] - 254s 9s/step - loss: 12.9867 - accuracy: 0.8952 - val_loss: 12.7947 - val_accuracy: 0.7333\n",
      "Epoch 74/79\n",
      "27/27 [==============================] - 253s 9s/step - loss: 12.7784 - accuracy: 0.9524 - val_loss: 12.7858 - val_accuracy: 0.7333\n",
      "Epoch 75/79\n",
      "27/27 [==============================] - 250s 9s/step - loss: 12.9425 - accuracy: 0.8762 - val_loss: 12.3959 - val_accuracy: 0.8000\n",
      "Epoch 76/79\n",
      "27/27 [==============================] - 251s 9s/step - loss: 12.8498 - accuracy: 0.8571 - val_loss: 12.5431 - val_accuracy: 0.8667\n",
      "Epoch 77/79\n",
      "27/27 [==============================] - 252s 9s/step - loss: 12.6406 - accuracy: 0.9238 - val_loss: 12.5627 - val_accuracy: 0.8000\n",
      "Epoch 78/79\n",
      "27/27 [==============================] - 253s 9s/step - loss: 12.6393 - accuracy: 0.9143 - val_loss: 12.5531 - val_accuracy: 0.8000\n",
      "Epoch 79/79\n",
      "27/27 [==============================] - 273s 10s/step - loss: 12.5598 - accuracy: 0.9524 - val_loss: 12.3071 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning Phase\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:300]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history_fine = model.fit(\n",
    "    AugmentedGenerator(train_generator),\n",
    "    validation_data=val_generator,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    epochs=history.epoch[-1] + 30,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9abfc2b3-2e5b-41c7-91b9-b518f08aee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('D:/Model_Main/Xception_net_client_odonata_cnn_tunned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "320a6bad-febb-49e9-8da7-6972ea1b6d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 24s 3s/step - loss: 12.3168 - accuracy: 0.9333\n",
      "Test Loss: 12.3168 | Test Accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {eval_results[0]:.4f} | Test Accuracy: {eval_results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "123ce889-186c-4d03-8693-0f384121838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels manually\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class_labels = {\n",
    "    0: 'Brown Marsh Hawk',\n",
    "    1: 'Common Picturewing',\n",
    "    2: 'Coral Cloudwing',\n",
    "    3: 'Green Marsh Hawk',\n",
    "    4: 'Lathrecista',\n",
    "    5: 'Libellago balus',\n",
    "    6: 'Macrodiplax cora',\n",
    "    7: 'Pantala',\n",
    "    8: 'Rambur',\n",
    "    9: 'Yellow Flutterer',\n",
    "    10: 'fluctuans',\n",
    "    11: 'fulvia Drury',\n",
    "    12: 'servilia Drury',\n",
    "    13: 'verticalis',\n",
    "    14: 'winged Forest Glory'\n",
    "    \n",
    "}\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('D:/Model_Main/Xception_net_client_odonata_cnn_tunned.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9fa87a14-fe24-4bff-ab17-ef4154bfd8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "The predicted class is: Yellow Flutterer\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image_path = 'D:/Less_data_test/odonata/b45.jpg'\n",
    "test_image = load_img(image_path, target_size=(480, 480))  # Match model input size\n",
    "test_image = img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis=0)\n",
    "test_image = preprocess_input(test_image)  # Use EfficientNetV2 preprocessing\n",
    "\n",
    "# Predict the class\n",
    "result = model.predict(test_image)\n",
    "predicted_class = np.argmax(result)\n",
    "\n",
    "# Get the predicted label\n",
    "prediction = class_labels[predicted_class]\n",
    "\n",
    "print(f'The predicted class is: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea78da7-a7eb-4122-aee5-649422bbe40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
